{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163e8c23",
   "metadata": {},
   "source": [
    "\n",
    "# Optimization: Gradient Descent, Lagrange Multipliers, and Convex Optimization Techniques\n",
    "\n",
    "## Overview\n",
    "Optimization is a fundamental aspect of mathematics and computational sciences, crucial in fields such as machine learning, economics, engineering, and operations research. This notebook covers:\n",
    "\n",
    "- **Gradient Descent**: An iterative optimization algorithm for finding the minimum of a function.\n",
    "- **Lagrange Multipliers**: A method to find local maxima and minima of a function subject to equality constraints.\n",
    "- **Convex Optimization**: Techniques for optimizing convex functions, which ensure global minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d315e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative method for minimizing a function by moving in the direction of the negative gradient. Itâ€™s widely used in machine learning to optimize cost functions.\n",
    "\n",
    "The update rule for gradient descent is:\n",
    "\n",
    "\\[ x_{n+1} = x_n - \\alpha \\nabla f(x_n) \\]\n",
    "\n",
    "where \\( \\alpha \\) is the learning rate and \\( \\nabla f(x) \\) is the gradient of the function.\n",
    "\n",
    "Let's implement gradient descent to find the minimum of a simple quadratic function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df896a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x = -1.9999961433486937, after 66 iterations\n",
      "Result from Gradient Descent: x = -1.9999961433486937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a sample function and its gradient\n",
    "def f(x):\n",
    "    return x**2 + 4*x + 4  # Example: f(x) = (x + 2)^2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2*x + 4  # Derivative of f(x)\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradient_descent(f_prime, initial_x, learning_rate=0.1, tolerance=1e-6, max_iterations=1000):\n",
    "    x = initial_x\n",
    "    for i in range(max_iterations):\n",
    "        grad = f_prime(x)\n",
    "        x_next = x - learning_rate * grad\n",
    "        if abs(x_next - x) < tolerance:\n",
    "            print(f\"Minimum found at x = {x_next}, after {i} iterations\")\n",
    "            return x_next\n",
    "        x = x_next\n",
    "    print(\"Max iterations reached without convergence.\")\n",
    "    return None\n",
    "\n",
    "# Run Gradient Descent\n",
    "initial_x = 10  # Starting point\n",
    "min_x = gradient_descent(f_prime, initial_x)\n",
    "print(f\"Result from Gradient Descent: x = {min_x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec280dc8",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Lagrange Multipliers\n",
    "\n",
    "Lagrange multipliers provide a method for optimizing a function subject to constraints. If we want to maximize or minimize \\( f(x, y) \\) subject to a constraint \\( g(x, y) = 0 \\), we introduce a new variable (multiplier) \\( \\lambda \\) and solve:\n",
    "\n",
    "\\[ \\nabla f(x, y) = \\lambda \\nabla g(x, y) \\]\n",
    "\n",
    "This method is useful in constrained optimization problems.\n",
    "\n",
    "Example: Find the maximum and minimum values of \\( f(x, y) = x^2 + y^2 \\) subject to \\( x + y = 1 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f146939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solutions with Lagrange multipliers: {lambda: -1, x: 1/2, y: 1/2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sympy import symbols, Eq, solve, diff\n",
    "\n",
    "# Define variables and functions\n",
    "x, y, lamb = symbols('x y lambda')\n",
    "f = x**2 + y**2  # Objective function\n",
    "g = x + y - 1    # Constraint\n",
    "\n",
    "# Define Lagrangian\n",
    "L = f + lamb * g\n",
    "\n",
    "# Compute partial derivatives\n",
    "L_x = diff(L, x)\n",
    "L_y = diff(L, y)\n",
    "L_lamb = diff(L, lamb)\n",
    "\n",
    "# Solve equations\n",
    "solutions = solve((L_x, L_y, L_lamb), (x, y, lamb))\n",
    "print(\"Solutions with Lagrange multipliers:\", solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8338be6",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Convex Optimization\n",
    "\n",
    "Convex optimization involves minimizing a convex function, where any local minimum is also a global minimum. Common methods include quadratic programming and linear programming.\n",
    "\n",
    "Let's use `scipy.optimize` to solve a convex optimization problem:\n",
    "\n",
    "Minimize \\( f(x, y) = x^2 + y^2 \\) subject to \\( x + y \\geq 1 \\) and \\( x, y \\geq 0 \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db33c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convex Optimization Result:\n",
      " message: Optimization terminated successfully\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.5\n",
      "       x: [ 5.000e-01  5.000e-01]\n",
      "     nit: 1\n",
      "     jac: [ 1.000e+00  1.000e+00]\n",
      "    nfev: 3\n",
      "    njev: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the objective function\n",
    "def objective(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# Define constraints and bounds\n",
    "constraints = {'type': 'ineq', 'fun': lambda x: x[0] + x[1] - 1}\n",
    "bounds = [(0, None), (0, None)]  # x, y >= 0\n",
    "\n",
    "# Solve the optimization problem\n",
    "result = minimize(objective, [0.5, 0.5], bounds=bounds, constraints=constraints)\n",
    "print(\"Convex Optimization Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b7457",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "- **Gradient Descent**: A fundamental iterative optimization method for finding minima.\n",
    "- **Lagrange Multipliers**: An approach to optimize functions with equality constraints.\n",
    "- **Convex Optimization**: Techniques to solve convex problems ensuring global minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab46de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
